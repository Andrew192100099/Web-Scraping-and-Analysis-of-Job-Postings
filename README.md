# ğŸ€„ Web Scraping and Analysis of Job Postings

From scattered online listings â†’ to clear insights for job seekers & recruiters.
_____________
## âœ¨ The Story

When I first looked at LinkedIn job postings, one thing became clear:
the data was everywhere â€” thousands of listings, messy titles, scattered skills, and no simple way to answer real career questions.

**So I set out on a mission:**

**1ï¸âƒ£ Scrape the data using Selenium â€”** navigating LinkedInâ€™s structure to collect job postings at scale.

**2ï¸âƒ£ Organize the chaos â€”** turning raw job titles and descriptions into structured datasets.

**3ï¸âƒ£ Ask business-driven questions â€”** the kind that job seekers, recruiters, and HR professionals really want answers to.

***And slowly, the patterns began to emerge...***
______

## ğŸ’¡ The Business Questions We Asked

**â” Which technical skills are most in demand across industries?**

**â” Do specific cities favor certain tools or languages?**

**â” Whatâ€™s the overlap of tools like Python, SQL, and Power BI in job postings?**

**â” How do job descriptions differ between roles (Data Analyst, Data Scientist, BI Specialist)?**
________________

## ğŸ” How We Answered Them

**âœ” Extracted skills from job titles using custom keyword detection (Python, SQL, Excel, Power BI, etc.).**
**âœ” Grouped and counted skills city by city to see what employers look for in different markets.**
**âœ” Built frequency rankings with Pythonâ€™s Counter to identify the top 8 skills per city.**
**âœ” Exported everything into structured CSV reports â€” easy to reuse and share.**
**âœ” Highlighted insights with visuals & summary tables for decision-making.**
_________________________

## ğŸ¯ Key Insights (Highlights from the Analysis)

**â¢ Python & SQL were the clear leaders across most job postings.**

**â¢ Power BI and Tableau dominated business-focused roles.**

**â¢ Excel remains a foundational tool â€” often required alongside advanced skills.**

**â¢ Skill preferences varied by city â€” some leaned heavily on BI tools, while others emphasized programming.**

**â¢ For cities with fewer postings, Data Analysis served as the default baseline skill.**
_______________________

## ğŸ› ï¸ Technical Journey

**ğŸ“ Web Scraping:** *Selenium for LinkedIn navigation & job extraction.*

**ğŸ“ Data Cleaning:** *Pandas to remove duplicates, normalize case, and standardize formats.*

**ğŸ“ Skill Extraction:** *Custom keyword dictionaries to capture core tech stacks.*

**ğŸ“ Analysis:** *Groupby, Counter, and Pandas operations to surface insights.*

**ğŸ“ Visualization:** *Matplotlib & Seaborn to highlight trends.*

**ğŸ“ Export:** *Clean CSV reports (linkedin_jobs.csv) for sharing & reuse.*
________________

## ğŸ“Œ Why This Project Matters

*This project wasnâ€™t just about scraping job ads*
**It was about transforming unstructured data into business intelligence:**

**âœ” Helping job seekers know where to focus their upskilling**

**âœ” Giving recruiters a sharper view of the talent landscape**

**âœ” Equipping businesses with knowledge of in-demand skills across regions**

***In short:*** *from raw data â†’ to career strategy*
